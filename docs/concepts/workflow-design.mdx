---
title:  Workflow Design
---
Designing efficient and effective workflows is a critical aspect of using ControlFlow to build AI-powered applications. A well-designed workflow enables smooth execution, clear task dependencies, and seamless integration with normal Python code. This document provides a comprehensive guide on designing workflows in ControlFlow, covering the various APIs for defining workflows, execution modes, handling dependencies, error handling, observability, and best practices.

## Functional and imperative APIs
ControlFlow provides both functional and imperative APIs for defining tasks and workflows. The functional API uses task-decorated functions (`@task`) to define tasks, while the imperative API uses the `Task` class to create task objects. Similarly, the `@flow` decorator and the `Flow` class can be used to define flows.

In general, most users will start with the functional API for flows (`@flow`) and imperative API for tasks (`Task`), as this is the most intuitive and concise way to define recognizable workflows. However, the choice between the two APIs will depend on your workflow requirements and personal preference. Both are fully supported.

Here's an example that shows the same workflow, written three different ways: with the fully imperative API, the fully functional API, and a mix of the two:
<CodeGroup>
```python Fully imperative API
from controlflow import Flow, Task


with Flow(name='poem_flow') as poem_flow:

    name = Task(
        objective="Get the user's name", 
        result_type=str, 
        user_access=True)

    poem = Task(
        objective="Write a short poem about the user and provided topic",
        result_type=str,
        context={"name": name, "topic": "sunset"},
    )


poem_flow.run()
print(poem.result)

# John Doe, across the days
# You shine with vibrant rays.
# ...
```
```python Fully functional API
from controlflow import flow, task


@task(user_access=True)
def get_name() -> str:
    """Get the user's name."""
    pass


@task
def write_poem(name: str, topic:str) -> str:
    """Write a short poem about the user and provided topic."""
    pass


@flow
def poem_flow(topic:str):
    name = get_name()
    poem = write_poem(name)
    return poem


print(poem_flow(topic="sunset"))

# John Doe, across the days
# You shine with vibrant rays.
# ...
```

```python Mixed APIs
from controlflow import flow, Task


@flow
def poem_flow(topic:str):

    name = Task(
        objective="Get the user's name", 
        result_type=str, 
        user_access=True)

    poem = Task(
        objective="Write a short poem about the user and provided topic",
        result_type=str,
        context={"name": name, "topic": topic},
    )

    return poem
    

print(poem_flow(topic="sunset"))

# John Doe, across the days
# You shine with vibrant rays.
# ...
```
</CodeGroup>


### Which API should I use?

<Tip>
Most users should start with the functional `@flow` decorator and imperative `Task` class, though the choice of whether to use `Task` or `@task` is ultimately a matter of personal preference and workflow requirements.
</Tip>

#### Flows
Users should almost always use the `@flow` decorator to create flows. It provides a simple and concise way to define flows using Python functions that encapsulate all logic. The `@flow` decorator automatically infers the structure of the flow and its tasks, making it easy to define and run workflows. The imperative `Flow` class is available for advanced users who need more control over flow creation and execution, but requires more boilerplate.
#### Tasks
For tasks, the choice between the functional and imperative APIs will depend on your workflow requirements and personal preference.

Most users should start with the imperative `Task` class. This approach leans in to an object-oriented style of programming, where tasks are explicitly defined and configured. This can be beneficial for users who prefer a more explicit and structured approach to defining tasks, with fine-grained control over task properties and behavior. 

The functional `@task` decorator creates tasks that look and behave like functions, making them relatively easy to define and use. The functional API is especially useful for prototyping, simple workflows, and tasks that do not have complex dependencies. This is because the functional API is eagerly executed by default. On the one hand this simplifies workflow design, but it also prevents agents from optimizing their work based on knowledge of the entire workflow.

### Advantages of the imperative API
The imperative API, using the `Task` class, offers the following advantages:

1. **Explicit control over task definition**: The `Task` class allows you to specify detailed objectives, instructions, agents, context, and result types for each task, providing fine-grained control over task behavior.

2. **Dynamic task creation**: Tasks can be created dynamically based on runtime conditions or data, enabling more flexible and dynamic workflows.

3. **Lazy execution**: The imperative API provides access to fine-grained control over task execution, dependencies, and error handling, allowing you to customize the workflow behavior as needed.


### Advantages of the functional API
The functional `@task` API offers several advantages:

1. **Automatic inference of task properties**: The `@task` decorator automatically infers task properties such as the objective, instructions, context, and result type from the function definition, reducing the need for explicit configuration.

2. **Eager execution**: Task-decorated functions are executed eagerly by default, which can be beneficial for some workflows and provide a more intuitive programming experience.

### Mixing functional and imperative APIs
ControlFlow allows you to mix functional and imperative APIs within a workflow, leveraging the strengths of both approaches. Most commonly, you'll seamlessly combine `@flow`-decorated functions with `Task` objects, but you can switch APIs at any time.

Here's an example that demonstrates mixing functional and imperative tasks:

```python
from controlflow import flow, task, Task

@task
def preprocess_data(raw_data: str) -> pd.DataFrame:
    """Preprocess the raw data and return a cleaned DataFrame."""
    pass

@flow
def my_flow():
    raw_data = load_raw_data()
    cleaned_data = preprocess_data(raw_data)

    analysis_task = Task(
        objective="Perform exploratory data analysis",
        context={"data": cleaned_data},
        result_type=dict,
    )

    insights = analysis_task.run()
    generate_report(insights)
```

In this example, the `preprocess_data` task is defined using the `@task` decorator, while the `analysis_task` is created imperatively using the `Task` class. The `preprocess_data` task is executed eagerly, and its result (`cleaned_data`) is passed as context to the `analysis_task`.

By mixing functional and imperative tasks, you can design workflows that are both expressive and flexible, allowing you to leverage the strengths of each approach as needed.

## Execution modes

ControlFlow supports two execution modes: eager execution and lazy execution. Understanding these modes is essential for controlling the behavior of your workflows. 

### Eager execution

Eager mode is the default for the functional API. In this mode, flows and tasks are executed immediately. When a `@task`-decorated function is called, it is run by an AI agent right away and its result is returned. When a `@flow`-decorated function is called, it executes the function, runs every task that was created inside the flow (whether functional or imperative) and returns the results of any returned tasks.

Here's an example of eager execution:

```python
from controlflow import flow, task

@task
def write_poem(topic: str) -> str:
    """Write a short poem about the given topic."""
    pass

@flow
def my_flow(topic:str):
    
    # the poem task is immediately given to an AI agent for execution
    # and the result is returned
    poem = write_poem(topic)
    return poem

my_flow("sunset")
```

In this example, the `write_poem` task is executed by an AI agent as soon as its function is called. The AI agent generates a short poem based on the provided topic, and the generated poem is returned as the `poem` variable.

Because eager execution returns the result of each task, it makes it easy to mix task-decorated functions with normal Python code seamlessly, enabling you to use standard Python control flow statements, such as conditionals and loops, to control the execution of tasks.

### Lazy execution

Lazy execution means that tasks are not executed when they are created. Instead, ContorlFlow builds a directed acyclic graph (DAG) of tasks and their dependencies, and executes them only when necessary. The advantage of this approach is that knowledge of the entire graph, including potentially future work, can be used by agents to optimize their work. It also defers potentially expensive work (in terms of time or resources) until it is actually needed.

Lazy execution is the only mode available for the imperative API, as imperative tasks must be run explicitly. You can also run functional tasks lazily by passing `lazy=True` to the `@task` decorator or `lazy_=True` when calling the task. In lazy mode, `@task` functions return a `Task` object.

Here's an example of lazy task execution:

```python
from controlflow import flow, task

# this task will always run lazily
@task(lazy=True)
def analyze_data(data: pd.DataFrame) -> dict:
    """Analyze the given data and return insights."""
    pass

@task
def generate_report(insights: dict) -> str:
    """Generate a report based on the provided insights."""
    pass

@flow
def my_flow(data):
    
    # analyze_data runs lazily, so the `insights` variable is a Task 
    # object, as if it had been created with the imperative API
    insights = analyze_data(data)

    # `report` is a Task object because generate_report is being called lazily
    report = generate_report(insights, lazy_=True)
    return report

result = my_flow()
```

In this example, the `analyze_data` and `generate_report` tasks are defined using the `@task` decorator, but their execution is deferred. `analyze_data` is defined lazily; `generate_report` is lazy on for this specific call.

Flows can also be run lazily. In this case, a `Flow` object is returned when calling the flow function. Note that this is a very advanced feature and most users will not use it.

#### Why use lazy execution?

Lazy execution is particularly useful in scenarios where you want to define the structure and dependencies of tasks upfront but delay their actual execution. This can be beneficial for planning and optimizing the execution of complex workflows. When tasks are run eagerly, the Agents can only see the workflow that's been defined up to that point. When tasks with dependencies are run lazily, the agent can see the entire workflow and optimize its execution of early task in order to produce the output it needs for later tasks. This can lead to more efficient and precise execution of the workflow.

In addition, lazy execution allows you to exercise more precise control over how tasks are executed. When you run an eagerly executed task, ControlFlow works autonomously to complete the task and all its upstream dependencies. This is similar to calling `run()` on any imperative task. However, the imperative API gives you access to methods like `run_once()` (only run an agent for one turn) and even choosing which agent to run the task with, which can be useful for debugging and optimization.

Remember that lazy execution is the only way to run tasks in the imperative API, so if you need to use imperative tasks, you'll be using lazy execution by default. Eager execution is the default for functional tasks because it is more intuitive for users expecting typical Python behavior when calling a function. 

#### When are lazy tasks run?

Lazily-executed tasks are run under the following conditions:
1. When their `run()` method is called.
2. When they are an upstream dependency of another task that is run (whether eagerly or lazily)
3. At the end of an eagerly-executed flow, if they were created in that flow.

## Task dependencies and execution order
ControlFlow provides mechanisms to define dependencies between tasks and control their execution order. Tasks can have upstream dependencies, meaning they can only be executed after their dependent tasks have completed successfully.

In this way, tasks form a DAG of operations. ControlFlow can take advantage of this DAG to help agents optimize the execution of tasks, ensuring that they are run in the correct order and that the results of upstream tasks are available to downstream tasks.

Note that eager execution minimizes the ability of the framework to optimize the DAG; this is the tradeoff for the simplicity of eager execution. Lazy execution allows the framework to optimize the DAG, but it requires more explicit control from the user.

### Upstream dependencies

Tasks can not be completed until their upstream dependencies are complete. An upstream dependency is any task that the current task depends on.

There are two ways to define upstream dependencies:
- `depends_on`: A list of tasks that the current task depends on. The task will not be executed until all of its dependencies have been completed. This enforces a strict order of execution.
- `context`: Any values in context are considered inputs to the task, so the task will not be executed until any other tasks in the context have been completed.

When you call a `@task`-decorated function with arguments, those values are automatically added to its context (including any inferred dependencies).

Here's an example that demonstrates task dependencies:

```python
from controlflow import flow, task, Task

@task
def task1():
    """Perform task 1."""
    pass

@task
def task2(result1):
    """Perform task 2 using the result from task 1."""
    pass

@flow
def my_flow():
    result1 = task1()
    result2 = task2(result1)

    task3 = Task(
        objective="Perform task 3",
        depends_on=[result2],
    )

    task4 = Task(
        objective="Perform task 4",
        context=dict(result3=task3),
    )

    task4.run()
```
In this example, `task2` depends on `task1`, as it takes the result of `task1` as an argument. `task3` is created imperatively and specifies `result2` as a dependency using the `depends_on` parameter. `task4` is also created imperatively and uses the result of `task3` as context.


ControlFlow ensures that tasks are executed in the correct order based on their dependencies. Tasks with unresolved dependencies will not be executed until all their upstream tasks have completed successfully.

### Parent and subtask dependencies

Another way to define dependencies is to use subtasks. Subtasks are tasks that are created within the context of another task and are automatically considered dependencies of that parent task. This means that a parent task can not be marked as complete until all of its subtasks have been completed.

A subtask can be created in two ways:
- By creating a task within the context of another task.
- By passing a task as the `parent` argument of another task.

In general, the context approach is more versatile since it works with both the functional and imperative APIs:

```python
from controlflow import flow, Task

@flow
def my_flow():
    with Task('Write a poem') as write_poem:
        verse_1 = Task('Write the first verse')
        verse_2 = Task('Write the second verse')
        verse_3 = Task('Write the third verse')
        title = Task('Write the title')

    return write_poem
```

In the above example, `write_poem` is the parent task, and `verse_1`, `verse_2`, `verse_3`, and `title` are subtasks. The parent task considers all subtasks as dependencies, so it will not be marked as complete until all subtasks have been completed. However, this also means that we only have to run the `write_poem` task to execute all subtasks, since tasks force their upstream dependencies to be run. That's why only the `write_poem` task is returned from the flow, where it will be run via eager execution.

## Error handling
ControlFlow provides mechanisms to handle errors and exceptions that may occur during task execution.

In eager execution mode, if a task fails and raises an exception, you can catch and handle the exception using Python's standard exception handling techniques, such as `try`-`except` blocks.

Here's an example of error handling in eager mode:

```python
from controlflow import flow, task

@task
def divide_numbers(a: int, b: int) -> float:
    """Divide two numbers."""
    pass

@flow
def my_flow():
    try:
        result = divide_numbers(10, 0)
        print(result)
    except ValueError as e:
        print(f"Error: {str(e)}")
```

In this example, if the `divide_numbers` task fails because the agent recognizes that it can't divide by zero, it will raise a `ValueError`. The exception is caught in the `except` block, and an error message is printed.

In lazy execution mode, exceptions are not raised immediately but are propagated through the task DAG. When the flow is executed using the `run()` method, any exceptions that occurred during task execution will be raised at that point.

## Observability and orchestration
ControlFlow integrates with Prefect, a popular workflow orchestration tool, to provide observability and orchestration capabilities for your workflows.

Under the hood, ControlFlow tasks and flows are modeled as Prefect tasks and flows, allowing you to leverage Prefect's features for monitoring, logging, and orchestration. This integration enables you to track the execution status of tasks, monitor their progress, and access detailed logs and reports.

By default, ControlFlow configures Prefect to use a local SQLite database for storage and a ephemeral Prefect server for orchestration. You can customize the Prefect configuration to use different storage backends and orchestration setups based on your requirements.

To access the Prefect UI and view the status and logs of your workflows, you can start the Prefect server and open the UI in your web browser:

```bash
prefect server start
```

Once the server is running, you can open the Prefect UI by navigating to `http://localhost:4200` in your web browser. The UI provides a visual interface to monitor the execution of your workflows, view task statuses, and access logs and reports.

## Best practices
When designing workflows in ControlFlow, consider the following best practices:

1. **Define clear task objectives**: Ensure that each task has a clear and specific objective. Provide detailed instructions and context to guide the AI agent in completing the task effectively.

2. **Choose the appropriate API**: Decide whether to use the functional or imperative API based on your workflow requirements. The functional API is suitable for simple workflows and quick prototyping, while the imperative API is beneficial for more complex workflows and advanced customization.

3. **Modularize tasks**: Break down complex tasks into smaller, modular subtasks. This promotes reusability, maintainability, and easier debugging.

4. **Handle errors gracefully**: Implement proper error handling mechanisms to deal with exceptions and task failures. Use `try`-`except` blocks in eager mode and handle exceptions appropriately in lazy mode.

5. **Leverage observability features**: Utilize the observability and monitoring capabilities provided by Prefect. Access the Prefect UI to track task execution, view logs, and gain insights into workflow performance.

6. **Test and debug workflows**: Thoroughly test your workflows to ensure they behave as expected. Use debugging techniques, such as logging statements and breakpoints, to identify and fix issues.

7. **Optimize performance**: Analyze and optimize the performance of your workflows. Identify bottlenecks, minimize unnecessary computations, and leverage parallelism and caching when applicable.

8. **Document and version control**: Maintain clear documentation for your workflows, including task descriptions, dependencies, and usage instructions. Use version control systems to track changes and collaborate effectively.

By following these best practices, you can design robust, efficient, and maintainable workflows in ControlFlow.
