Designing efficient and effective workflows is a critical aspect of using ControlFlow to build AI-powered applications. A well-designed workflow enables smooth execution, clear task dependencies, and seamless integration with normal Python code. This document provides a comprehensive guide on designing workflows in ControlFlow, covering execution modes, combining task types, handling dependencies, error handling, observability, and best practices.

## Functional and Imperative APIs
ControlFlow provides both functional and imperative APIs for defining tasks and workflows. The functional API uses task-decorated functions (`@task`) to define tasks, while the imperative API uses the `Task` class to create tasks imperatively. Similarly, the `@flow` decorator and the `Flow` class can be used to define flows.

<CodeGroup>
```python The functional API
from controlflow import flow, task


@task(user_access=True)
def get_name() -> str:
    """Get the user's name."""
    pass


@task
def write_poem(name: str) -> str:
    """Write a short poem about the user."""
    pass


@flow
def poem_flow():
    name = get_name()
    poem = write_poem(name)
    return poem


print(poem_flow())

# John Doe, across the days
# You shine with vibrant rays.
# ...
```

```python The imperative API
from controlflow import Flow, Task


with Flow(name='poem_flow') as poem_flow:
    name = Task(
        objective="Get the user's name", 
        result_type=str, 
        user_access=True)

    poem = Task(
        objective="Write a short poem about the user",
        result_type=str,
        context={"name": name},
    )


poem_flow.run()
print(poem.result)
```
```python Mixed APIs
from controlflow import flow, Task


@flow
def poem_flow()
    name = Task(
        objective="Get the user's name", 
        result_type=str, 
        user_access=True)

    poem = Task(
        objective="Write a short poem about the user",
        result_type=str,
        context={"name": name},
    )

    return poem
    

print(poem_flow())
```
</CodeGroup>

### Which Approach Should I Use?

<Tip>
**tldr:** Most users should use the functional `@flow` decorator, but the choice of whether to use `@task` or `Task` will be a matter of personal preference and workflow requirements.
</Tip>

Users should almost always use the `@flow` decorator to create flows. It provides a simple and concise way to define flows using Python functions that encapsulate all logic. The `@flow` decorator automatically infers the structure of the flow and its tasks, making it easy to define and run workflows.

For tasks, the choice between the functional and imperative APIs will depend entirely on your workflow requirements and personal preference.

Some users will feel that task functions look more natural when defined and called. Others may find the bodyless functions created by the `@task` decorator to be less readable and prefer the terse explicitness of the `Task` class. There is no superior choice between the two, and users should choose the API that best fits their workflow and coding style.

### Advantages of the Functional API
The functional `@task` API offers several advantages:

1. **Concise and readable code**: Task-decorated functions allow you to define tasks using familiar Python syntax, making the code more concise and easier to read.

2. **Automatic inference of task properties**: The `@task` decorator automatically infers task properties such as the objective, instructions, context, and result type from the function definition, reducing the need for explicit configuration.

3. **Eager execution**: Task-decorated functions are executed eagerly by default, which can be beneficial for some workflows, especially those that rely on task results. 

### Advantages of the Imperative API
The imperative API, using the `Task` class, offers the following advantages:

1. **Explicit control over task definition**: The `Task` class allows you to specify detailed objectives, instructions, agents, context, and result types for each task, providing fine-grained control over task behavior.

2. **Dynamic task creation**: Tasks can be created dynamically based on runtime conditions or data, enabling more flexible and dynamic workflows.

3. **Lazy execution**: The imperative API provides access to fine-grained control over task execution, dependencies, and error handling, allowing you to customize the workflow behavior as needed.

### Mixing Functional and Imperative APIs
ControlFlow allows you to mix functional and imperative APIs within a workflow, leveraging the strengths of both approaches. Most commonly, you'll seamlessly combine `@flow`-decorated functions with both `@task`-decorated functions and tasks created using the `Task` class.

Here's an example that demonstrates mixing functional and imperative tasks:

```python
from controlflow import flow, task, Task

@task
def preprocess_data(raw_data: str) -> pd.DataFrame:
    """Preprocess the raw data and return a cleaned DataFrame."""
    pass

@flow
def my_flow():
    raw_data = load_raw_data()
    cleaned_data = preprocess_data(raw_data)

    analysis_task = Task(
        objective="Perform exploratory data analysis",
        context={"data": cleaned_data},
        result_type=dict,
    )

    insights = analysis_task.run()
    generate_report(insights)
```

In this example, the `preprocess_data` task is defined using the `@task` decorator, while the `analysis_task` is created imperatively using the `Task` class. The `preprocess_data` task is executed eagerly, and its result (`cleaned_data`) is passed as context to the `analysis_task`.

By mixing functional and imperative tasks, you can design workflows that are both expressive and flexible, allowing you to leverage the strengths of each approach as needed.

## Execution Modes

ControlFlow supports two execution modes for the functional API: eager execution and lazy execution. Understanding these modes is essential for controlling the behavior of your workflows.

### Eager Execution

Eager execution is the default mode for flow- and task-decorated functions. When a `@task`-decorated function is called in eager mode, it is executed immediately by an AI agent. When a `@flow`-decorated function, it runs every task that was created inside the flow (whether functional or imperative) and resolves any returned tasks as their results.

Here's an example of eager execution:

```python
from controlflow import flow, task

@task
def write_poem(topic: str) -> str:
    """Write a short poem about the given topic."""
    pass

@flow
def my_flow():
    topic = "sunset"
    # the poem task is immediately given to an AI agent for execution
    # and the result is returned
    poem = write_poem(topic)
    print(poem)

my_flow()
```

In this example, the `write_poem` task is executed by an AI agent as soon as its function is called. The AI agent generates a short poem based on the provided topic, and the generated poem is returned as the `poem` variable.

Because eager execution returns the result of each task, it makes it easy to mix task-decorated functions with normal Python code seamlessly, enabling you to use standard Python control flow statements, such as conditionals and loops, to control the execution of tasks.

### Lazy Execution
Lazy execution is the only mode available for the imperative API, as imperative tasks are not run until you explicitly run them or one of their downstream dependencies. 

Lazy execution for the functional API is an advanced mode that allows you to defer the execution of task-decorated functions. Instead of executing tasks immediately, lazy execution converts `@task` functions into `Task` objects, which can be used to build up a directed acyclic graph (DAG) of tasks within the flow.

To run a functional task lazily, you can pass `lazy_=True` (note the trailing underscore) when calling it, or pass `lazy=True` to the `@task` decorator.

Here's an example of lazy task execution:

```python
from controlflow import flow, task

# this task will always run lazily
@task(lazy=True)
def analyze_data(data: pd.DataFrame) -> dict:
    """Analyze the given data and return insights."""
    pass

@task
def generate_report(insights: dict) -> str:
    """Generate a report based on the provided insights."""
    pass

@flow
def my_flow():
    data = load_data()
    # insights is a Task object, as if analyze_data had been created
    # with the imperative API
    insights = analyze_data(data)

    # `report` is a Task object because generate_report is being called lazily
    report = generate_report(insights, lazy_=True)
    return report

result = my_flow()
```

In this example, the `analyze_data` and `generate_report` tasks are defined using the `@task` decorator, but their execution is deferred. `analyze_data` is defined lazily; `generate_report` is lazy on for this specific call.



Flows can also be run lazily. In this case, a `Flow` object is returned when calling the flow function. Note that this is a very advanced feature and most users will not need to use it.

### Why Use Lazy Execution?

Lazy execution is particularly useful in scenarios where you want to define the structure and dependencies of tasks upfront but delay their actual execution. This can be beneficial for planning and optimizing the execution of complex workflows. When tasks are run eagerly, the Agents can only see the workflow that's been defined up to that point. When tasks with dependencies are run lazily, the agent can see the entire workflow and optimize its execution of early task in order to produce the output it needs for later tasks. This can lead to more efficient and precise execution of the workflow.

In addition, lazy execution allows you to exercise more precise control over how tasks are executed. When you run an eagerly executed task, ControlFlow works autonomously to complete the task and all its upstream dependencies. This is similar to calling `run()` on any imperative task. However, the imperative API gives you access to methods like `run_once()` (only run an agent for one turn) and even choosing which agent to run the task with, which can be useful for debugging and optimization.

Remember that lazy execution is the only way to run tasks in the imperative API, so if you need to use imperative tasks, you'll be using lazy execution by default. Eager execution is the default for functional tasks because it is more intuitive for users expecting typical Python behavior when calling a function. 

## Task Dependencies and Execution Order
ControlFlow provides mechanisms to define dependencies between tasks and control their execution order. Tasks can have upstream dependencies, meaning they can only be executed after their dependent tasks have completed successfully.

In this way, tasks form a DAG of operations. ControlFlow can take advantage of this DAG to help agents optimize the execution of tasks, ensuring that they are run in the correct order and that the results of upstream tasks are available to downstream tasks.

Note that eager execution minimizes the ability of the framework to optimize the DAG; this is the tradeoff for the simplicity of eager execution. Lazy execution allows the framework to optimize the DAG, but it requires more explicit control from the user.

### Upstream dependencies

Tasks can not be completed until their upstream dependencies are complete. An upstream dependency is any task that the current task depends on.

There are two ways to define upstream dependencies:
- `depends_on`: A list of tasks that the current task depends on. The task will not be executed until all of its dependencies have been completed. This enforces a strict order of execution.
- `context`: Any values in context are considered inputs to the task, so the task will not be executed until any other tasks in the context have been completed.

When you call a `@task`-decorated function with arguments, those values are automatically added to its context (including any inferred dependencies).

Here's an example that demonstrates task dependencies:

```python
from controlflow import flow, task, Task

@task
def task1():
    """Perform task 1."""
    pass

@task
def task2(result1):
    """Perform task 2 using the result from task 1."""
    pass

@flow
def my_flow():
    result1 = task1()
    result2 = task2(result1)

    task3 = Task(
        objective="Perform task 3",
        depends_on=[result2],
    )

    task4 = Task(
        objective="Perform task 4",
        context=dict(result3=task3),
    )

    task4.run()
```
In this example, `task2` depends on `task1`, as it takes the result of `task1` as an argument. `task3` is created imperatively and specifies `result2` as a dependency using the `depends_on` parameter. `task4` is also created imperatively and uses the result of `task3` as context.


ControlFlow ensures that tasks are executed in the correct order based on their dependencies. Tasks with unresolved dependencies will not be executed until all their upstream tasks have completed successfully.

### Subtask Dependencies

Another way to define dependencies is to use subtasks. Subtasks are tasks that are created within the context of another task and are automatically considered dependencies of the parent task. This means that a parent task can not be marked as complete until all of its subtasks have been completed.

However, as a convenience, subtasks can be marked skipped if an agent determines that they are not necessary.

A subtask can be created in two ways:
- By creating a task within the context of another task.
- By passing a task as the `parent` argument of another task.

In general, the context approach is more versatile since it works with both the functional and imperative APIs.

```python
from controlflow import flow, Task

@flow
def my_flow():
    with Task('Write a poem') as write_poem:
        verse_1 = Task('Write the first verse')
        verse_2 = Task('Write the second verse')
        verse_3 = Task('Write the third verse')
        title = Task('Write the title')

    return write_poem
```

In the above example, `write_poem` is the parent task, and `verse_1`, `verse_2`, `verse_3`, and `title` are subtasks. The parent task considers all subtasks as dependencies, so it will not be marked as complete until all subtasks have been completed. However, this also means that we only have to run the `write_poem` task to execute all subtasks, since tasks force their upstream dependencies to be run. That's why only the `write_poem` task is returned from the flow, where it will be run via eager execution.

## Error Handling
ControlFlow provides mechanisms to handle errors and exceptions that may occur during task execution.

In eager execution mode, if a task fails and raises an exception, you can catch and handle the exception using Python's standard exception handling techniques, such as `try`-`except` blocks.

Here's an example of error handling in eager mode:

```python
from controlflow import flow, task

@task
def divide_numbers(a: int, b: int) -> float:
    """Divide two numbers."""
    pass

@flow
def my_flow():
    try:
        result = divide_numbers(10, 0)
        print(result)
    except ValueError as e:
        print(f"Error: {str(e)}")
```

In this example, if the `divide_numbers` task fails because the agent recognizes that it can't divide by zero, it will raise a `ValueError`. The exception is caught in the `except` block, and an error message is printed.

In lazy execution mode, exceptions are not raised immediately but are propagated through the task DAG. When the flow is executed using the `run()` method, any exceptions that occurred during task execution will be raised at that point.

## Observability and Orchestration
ControlFlow integrates with Prefect, a popular workflow orchestration tool, to provide observability and orchestration capabilities for your workflows.

Under the hood, ControlFlow tasks and flows are modeled as Prefect tasks and flows, allowing you to leverage Prefect's features for monitoring, logging, and orchestration. This integration enables you to track the execution status of tasks, monitor their progress, and access detailed logs and reports.

By default, ControlFlow configures Prefect to use a local SQLite database for storage and a ephemeral Prefect server for orchestration. You can customize the Prefect configuration to use different storage backends and orchestration setups based on your requirements.

To access the Prefect UI and view the status and logs of your workflows, you can start the Prefect server and open the UI in your web browser:

```bash
prefect server start
```

Once the server is running, you can open the Prefect UI by navigating to `http://localhost:4200` in your web browser. The UI provides a visual interface to monitor the execution of your workflows, view task statuses, and access logs and reports.

## Best Practices
When designing workflows in ControlFlow, consider the following best practices:

1. **Define clear task objectives**: Ensure that each task has a clear and specific objective. Provide detailed instructions and context to guide the AI agent in completing the task effectively.

2. **Choose the appropriate API**: Decide whether to use the functional or imperative API based on your workflow requirements. The functional API is suitable for simple workflows and quick prototyping, while the imperative API is beneficial for more complex workflows and advanced customization.

3. **Modularize tasks**: Break down complex tasks into smaller, modular subtasks. This promotes reusability, maintainability, and easier debugging.

4. **Handle errors gracefully**: Implement proper error handling mechanisms to deal with exceptions and task failures. Use `try`-`except` blocks in eager mode and handle exceptions appropriately in lazy mode.

5. **Leverage observability features**: Utilize the observability and monitoring capabilities provided by Prefect. Access the Prefect UI to track task execution, view logs, and gain insights into workflow performance.

6. **Test and debug workflows**: Thoroughly test your workflows to ensure they behave as expected. Use debugging techniques, such as logging statements and breakpoints, to identify and fix issues.

7. **Optimize performance**: Analyze and optimize the performance of your workflows. Identify bottlenecks, minimize unnecessary computations, and leverage parallelism and caching when applicable.

8. **Document and version control**: Maintain clear documentation for your workflows, including task descriptions, dependencies, and usage instructions. Use version control systems to track changes and collaborate effectively.

By following these best practices, you can design robust, efficient, and maintainable workflows in ControlFlow.
